{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "***\n",
    "- [Introduction](#introduction)\n",
    "- [Part 1: Importing Libraries](#Importing_Libraries)\n",
    "- [Part 2: Importing Dataset](#Importing_Dataset)\n",
    "- [Part 3: Explore Dataset](#Explore_Dataset)\n",
    "- [Part 4: Explore Target Variable](#target)\n",
    "- [Part 5: Explore and Engineer Features](#features)\n",
    "    - [5.1. Continues Variables](#continues)\n",
    "    - [5.2. Categorical Variables](#cat)\n",
    "- [Part 6: Modeling](#modeling)\n",
    "    - [6.1. Split Dataset](#split)\n",
    "    - [6.2. Choosing Optimal Model](#choise)\n",
    "    - [6.3. Parameter Tuning for AdaBoost Classifier](#tuneA)\n",
    "    - [6.4. Parameter Tuning for Gradient Boosting Classifier](#tuneB)\n",
    "    - [6.5. Logistic Regression](#knn)\n",
    "    - [6.6. Random Forest](#forest)\n",
    "    - [6.7. Comparison of Classifiers](#compare)\n",
    "- [Part 7: Preprocessing of Test Data](#test)\n",
    "- [Part 8: Prediction and Submission](#pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<a id=\"introduction\"></a>\n",
    "\n",
    "In this project, I will apply supervised learning techniques and an analytical mind on data collected for the U.S. census to help CharityML (a fictitious charity organization) identify people most likely to donate to their cause. I will first explore the data to learn how the census data is recorded. Next, I will apply a series of transformations and preprocessing techniques to manipulate the data into a workable format. I will then evaluate several supervised learners of your choice on the data, and consider which is best suited for the solution. Afterwards, I will optimize the model selected and present it as a solution to CharityML. Finally, you will explore the chosen model and its predictions under the hood, to see just how well it's performing when considering the data it's given.\n",
    "\n",
    "The success of your model will be determined based on models AUC or area under the curve associated with ROC curves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Importing Libraries\n",
    "<a id=\"Importing_Libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# import useful libraries\n",
    "import numpy as np #\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from time import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# Import the supervised learning models from sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Importing Dataset\n",
    "<a id=\"Importing_Dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"census.csv\")\n",
    "data_test = pd.read_csv(\"test_census.csv\").drop('Unnamed: 0',1)\n",
    "#make a copy of data_train to overwrite during feature engineering\n",
    "train = data_train[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Explore Dataset\n",
    "<a id=\"Explore_Dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test set shape\n",
    "print(\"Train set shape:\", train.shape)\n",
    "print(\"Test set shape:\", data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first glance on train set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first glance on test data\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check info for train and test dataset\n",
    "data_train.info()\n",
    "print(\"----------------------------------\")\n",
    "data_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "according to the information on dataset, there are no missing values in train dataset, however, there are values missing in test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of records\n",
    "n_records = train.shape[0]\n",
    "\n",
    "# Print the results\n",
    "print(\"Total number of records: {}\".format(n_records))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Explore Target Variable\n",
    "<a id=\"target\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than \\$50,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore unique values for income, to use it below \n",
    "data_train.income.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records where individual's income is more than $50,000\n",
    "n_greater_50k = train[train.income == '>50K' ].shape[0]\n",
    "\n",
    "# Number of records where individual's income is at most $50,000\n",
    "n_at_most_50k = train[train.income == '<=50K' ].shape[0]\n",
    "\n",
    "# Percentage of individuals whose income is more than $50,000\n",
    "greater_percent = round((n_greater_50k/n_records)*100 ,2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Individuals making more than $50,000: {}\".format(n_greater_50k))\n",
    "print(\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\n",
    "print(\"Percentage of individuals making more than $50,000: {}%\".format(greater_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the algorithms can't work with categorical strings and transformation of string categories into numeric values is required. Since there are only two possible categories for this label (\"<=50K\" and \">50K\"), we can avoid using one-hot encoding and simply encode these two categories as 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform target into new variable called income:\n",
    "income=data_train.income.map({'<=50K': 0, '>50K':1})\n",
    "income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Explore and Engineer Features\n",
    "<a id=\"features\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to modeling stage it is very important to explore and preprocess features. Proper feature engineering can improve model performance significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many unique values each feature has:\n",
    "for column in data_train.columns:\n",
    "    print(column, len(train[column].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age, capital_gain, capital_loss, hours_per_week and education_num are continues features, rest are categorical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['workclass', 'education_level', 'marital-status', 'occupation', 'relationship', \n",
    "               'race', 'sex', 'native-country']\n",
    "continues = ['age', 'capital-gain', 'capital-loss', 'hours-per-week', 'education-num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each categorical features pring unique values:\n",
    "for column in categorical:\n",
    "    print(column, train[column].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary \n",
    "* **age**: continuous. \n",
    "* **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked. \n",
    "* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. \n",
    "* **education-num**: continuous. \n",
    "* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. \n",
    "* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. \n",
    "* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. \n",
    "* **race**: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other. \n",
    "* **sex**: Female, Male. \n",
    "* **capital-gain**: continuous. \n",
    "* **capital-loss**: continuous. \n",
    "* **hours-per-week**: continuous. \n",
    "* **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Continues Variables\n",
    "<a id=\"continues\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number.  Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized.  \n",
    "\n",
    "We will perform three steps to preprocess continues data:\n",
    "    - check skew of data and perform log transform on skewed variables\n",
    "    - find outliers\n",
    "    - normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Skewed Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot disctribution and check skewness:\n",
    "for column in continues:\n",
    "    a = sns.FacetGrid(train, aspect=4 )\n",
    "    a.map(sns.kdeplot, column, shade= True )\n",
    "    a.add_legend()\n",
    "    print('Skew for ',str(column), train[column].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From claculated skew and plots it can be stated that capital-gain and capital-loss features are highly skewed. For highly-skewed feature distributions, it is common practice to apply a logarithmic transformation on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation, however: the logarithm of `0` is undefined, so we must translate the values by a small amount above `0` to apply the the logarithm successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed = ['capital-gain', 'capital-loss']\n",
    "# Log-transform the skewed features (create function to use later for test set)\n",
    "def log_transform(data):\n",
    "    return data[skewed].apply(lambda x: np.log(x + 1))\n",
    "    \n",
    "train[skewed] = log_transform(train)\n",
    "\n",
    "# Visualize the new log distributions\n",
    "for column in skewed:\n",
    "    a = sns.FacetGrid(train, aspect=4 )\n",
    "    a.map(sns.kdeplot, column, shade= True )\n",
    "    a.add_legend()\n",
    "    print('Skew for ',str(column), train[column].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying log transformation on capital-gain and capital-loss helped to reduce skew, however, skew is still pretty high for this two features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers can shift decision boundry for linear models significanlty, thats why is it inportant to handle them. Tree models are not sensitive for ourliers, but shifting outliers will not effect them in any way, so we will perform shifting for all models.  \n",
    "In order to check for outliers we will make box plots for continues features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in continues:\n",
    "    sns.boxplot(train[column])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs it can be seen that all continues features have outliers. Outliers can be a problem for linear model performance. I will leave this outliers without preprocessing them, as they do not effect tree based models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Numerical Features\n",
    "Normalizing numerical features is important for a lot of algorithms. Proper feature scaling can help gradient descent to converge faster. Moreover, algorithms that are using eucledian disctance (such as K-meand and KNN) and regression coefficients of linear models  are effected by scaling.  \n",
    "Applying scaling to the data does not change the shape of each feature's distribution, however, normalization ensures that each feature is treated equally when applying supervised learners.  \n",
    "There are multiple ways to scale features: like MinMaxScaler, StandardScaler, RobustScaler and etc.  \n",
    "We will use [`MinMaxScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#normalizing numerical features. Create function to use later on test data\n",
    "\n",
    "def normalize(data):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    data=scaler.fit_transform(data[continues])\n",
    "    return data\n",
    "\n",
    "train[continues]= normalize(train)\n",
    "train.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Categorical Variables\n",
    "<a id=\"cat\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called categorical variables) be converted.  \n",
    "\n",
    "There are multiple ways to perform encoding:\n",
    "\n",
    "1. One-hot encoding:  \n",
    "keeps all information about variable, but create a lot of new features  \n",
    "\n",
    "2. Binary encoding:  \n",
    "keeps all information about variable, creates new dimensions, but less than one-hot encoder        \n",
    "             \n",
    "3. Ordinal numbering encoding:    \n",
    "keeps semantical info about variable, but treats 2-1 as 3-2 and can be missleading, doesn't create new dimensions  \n",
    "\n",
    "4. Frequency encoding:  \n",
    "doesn't create new dimensions, but depends on frequency rather than relation with target and that can lead to wrong predictions  \n",
    "\n",
    "5. Target guided encoding: ordinal, mean, and probability ratio encoding, Weight of evidence:  \n",
    "creates monotonic relationship between variable and target, but can cause overfitting \n",
    "\n",
    "All of this models have advantages and disadvantagies, and performance of particular encoding strategy for categorical features depends on amount of instances and features in data set, variable type and chosen prediction model. \n",
    "\n",
    "According to the info about unique values for categorical variables in current dataset, there are not so many unique values for each feature. Therefore we will use one-hot encoding strategy, where all categories of variables are converted into separate features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode thedata using pandas.get_dummies()\n",
    "features_final = pd.get_dummies(train.drop(['income'],1))\n",
    "\n",
    "# Print the number of features after one-hot encoding\n",
    "encoded = list(features_final.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlation between features: \n",
    "data = pd.concat([features_final, income], axis =1)\n",
    "plt.figure(figsize=(30,28))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(data.corr(),linewidths=0.1,vmax=1.0, \n",
    "            square=True,linecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information discovered about dataset:\n",
    "- dataset size: (45222, 103)\n",
    "- feature correlation - mostly low --> low linear relation between variables\n",
    "- income is unballanced, however, not so much, enough data for both classes\n",
    "- most of the features have low correlation with target, but around 10% of features have more than 0.4 correlation with target\n",
    "- continues features have outliers - negative effect on linear models performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Modeling\n",
    "<a id=\"Importing_Libraries\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chaper, we will investigate six different algorithms that are suitable for Charity ML prediction, and determine which 3 are the best at modeling the data. Four of these algorithms will be supervised learners of choice for further parameter tuning and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Split Dataset\n",
    "<a id=\"split\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features_final, \n",
    "                                                    income, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_val.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Choosing optimal model\n",
    "<a id=\"choise\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to information discovered about dataset:\n",
    "\n",
    "1. Hypothesis about computation complexity of models: \n",
    "    - Dataset is more than 10000 samples - too big to use SVC due to hight computation complexity of the model: O(n_samples^2 * n_features) for RBF kernel \n",
    "    - Random forest - low computation complexity (trees can be constracted in parallel)\n",
    "    - KNN - very big computation complexity in test time (each test point needs to be compared with each point in training set to find nearest neighbor) \n",
    "    - Gradient Boost - computation complexity depends on n_estimators, as it is sequential model\n",
    "    - Decision tree, logictic regression and Naive Bayes - low computation complexity \n",
    "\n",
    "    \n",
    "2. Hypothesis about model performance:\n",
    "    - Linear models - medium performance as there is not so much correlation in dataset \n",
    "    - Ensemble models are in general good performers, especially with optimal parameters\n",
    "    - KNN - usually good performer\n",
    "    - Decision tree - in general good performer, but has overfitting problem\n",
    "    - Naive Bayes - low performance due to assumption for independence in data - not realistic in real world\n",
    "    \n",
    "According to all mentioned above, my assumption is that ensemble models will outperform all other models in performance and still provide feasible comptutation complexity.   \n",
    "I will check 3 of them : AdaBoost, Random Forest, and Gradient Boosting.\n",
    "However, I will also check 3 other models to confirm correctness of my assumptions: KNN, Logictic regression and Naive Bayes.\n",
    "I will leave SVC out of scope due to very high computation complexity.\n",
    "\n",
    "1. Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting): \n",
    "    - Pros: no overfitting; no need to worry about data linearity; runs efficiently on large datasets \n",
    "    - Cons: only predicts in range of data available in train set; doesn't train well on small datasets; training time can be huge as needs to train multiple trees, but possible to reduce running parallel processing \n",
    "    - Dataset is big enough to use ensemble; can deal with non-linear data; in general ensemble models are top performance models \n",
    "       \n",
    "2. Logistic regression: \n",
    "    - Pros: simple to understand and explain model; fast to train; good to predict probability of events\n",
    "    - Cons: Needs linear inputs --> extensive feature engineering; suffers from ourliers   \n",
    "    - Performance will be lower as there is not so much correlation between variables in dataset  \n",
    "\n",
    "3. K-Nearest Neighbors:\n",
    "    - Pros: lazy training, good performance\n",
    "    - Cons: slow in predicting phase, susceptible to high dimensional dataset    \n",
    "    - Good relative performance, but will be slow in testing time. I would not use this algorithm in applications that need fast decision, but for this project this information is missing from initial conditions, so I decided to try this algorithm as well\n",
    "    \n",
    "3. Gaussian Naive Bayes (GaussianNB):\n",
    "   - Pros: performs well with categorical variables; converges fast; good with moderate and large datasets\n",
    "   - Cons: independence assumption- correlated features effect performance    \n",
    "   - Can deal with medium and large datasets and is fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly evaluate the performance of each model chosen, it's important that you create a training and predicting pipeline that allows to quickly and effectively train models using various sizes of training data and perform predictions on the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(results):\n",
    "    \"\"\"\n",
    "    Visualization code to display results of various learners.\n",
    "    \n",
    "    inputs:\n",
    "      - learners: a list of supervised learners\n",
    "      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n",
    "      - accuracy: The score for the naive predictor\n",
    "      - f1: The score for the naive predictor\n",
    "    \"\"\"\n",
    "  \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(2, 2, figsize = (18,10))\n",
    "\n",
    "    # Constants\n",
    "    bar_width = 1\n",
    "    colors = ['r','g','b','c', 'm', 'y']\n",
    "    \n",
    "    # Super loop to plot four panels of data\n",
    "    for k, learner in enumerate(results.keys()):\n",
    "        for j, metric in enumerate(['train_time', 'roc_train',  'pred_time', 'roc_test']):\n",
    "\n",
    "                ax[j//2, j%2].bar(k*bar_width, results[learner][metric], width = bar_width, color = colors[k])\n",
    "    \n",
    "    # Add unique y-labels\n",
    "    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n",
    "    ax[0, 1].set_ylabel(\"ROC-AUC Score\")\n",
    "    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n",
    "    ax[1, 1].set_ylabel(\"ROC-AUC Score\")\n",
    "    \n",
    "    # Add titles\n",
    "    ax[0, 0].set_title(\"Model Training\")\n",
    "    ax[0, 1].set_title(\"ROC-AUC Score on Training Subset\")\n",
    "    ax[1, 0].set_title(\"Model Predicting\")\n",
    "    ax[1, 1].set_title(\"ROC-AUC Score on Testing Set\")\n",
    "       \n",
    "    # Set y-limits for score panels\n",
    "    ax[0, 1].set_ylim((0, 1))\n",
    "    ax[1, 1].set_ylim((0, 1))\n",
    "\n",
    "    # Create patches for the legend\n",
    "    patches = []\n",
    "    for i, learner in enumerate(results.keys()):\n",
    "        patches.append(mpatches.Patch(color = colors[i], label = learner))\n",
    "    ax[1, 0].legend(handles = patches)\n",
    "    \n",
    "    # Aesthetics\n",
    "    plt.suptitle(\"Performance Metrics for Supervised Learning Models\", fontsize = 16, y = 1.10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display inline matplotlib plots with IPython\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "def train_predict(learner, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_val: features testing set\n",
    "       - y_val: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Fit the learner to the training data \n",
    "    start = time() # Get start time\n",
    "    learner.fit(X_train, y_train)\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # Get the predictions on the test set(X_test),\n",
    "    #       then get predictions on the first 300 training samples(X_train) using .predict()\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_val)\n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # Compute accuracy on the first 300 training samples which is y_train[:300]\n",
    "    results['roc_train'] = roc_auc_score(y_train[:300], predictions_train)\n",
    "        \n",
    "    # Compute accuracy on test set using accuracy_score()\n",
    "    results['roc_test'] = roc_auc_score(y_val, predictions_test)\n",
    "              \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state =42\n",
    "n_estimators =100\n",
    "\n",
    "# Initialize the three models\n",
    "clf_A = GaussianNB()\n",
    "clf_B = KNeighborsClassifier()\n",
    "clf_C = LogisticRegression(random_state= random_state)\n",
    "clf_D = RandomForestClassifier(random_state= random_state, n_estimators = n_estimators)\n",
    "clf_E = GradientBoostingClassifier(n_estimators = n_estimators, random_state = random_state)\n",
    "clf_F = AdaBoostClassifier(n_estimators = n_estimators, random_state = random_state)\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C, clf_D, clf_E, clf_F]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    results[clf_name] = train_predict(clf, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run metrics visualization for the three supervised learning models chosen\n",
    "evaluate(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Optimal Models: Summary\n",
    "\n",
    "1. Metrics:   \n",
    "    - As expected good predictors are ensemble models. All off them provide high F-score for training/test set. Random forest has lower F-score for test data, however, very high F-score for train data. This shows that default Random Forest Classier is overfitting training data. However, by choosing correct parameters, Random Forest model will have better balance between train/test data  \n",
    "\n",
    "2. Comutation complexity:  \n",
    "    - NaiveBayes, KNN, Logistic regression and Random Forest are the fastest models to train\n",
    "    - KNN is very slow in prediction, other models are relatively fast \n",
    "    - From ensemble models that use default parameters Random forest is the fastest, as it can train multiple trees at the same time. Gradient boosting classifier seams to be the slowest, as trees are trained using cascaded approach. However, all mentined above is subjective information as computation complexity depends on model parameters. \n",
    "    \n",
    "3. Suitability for the data:\n",
    "    - dataset size: (45222, 103)\n",
    "    - as correlation between features are relatively low and also ensemple models perform significantly better, I assume that non-linear relationships between features exsist in this dataset and they are important for correct prediction. The dataset is already 103 dimensions, so additional non-linear features will expand feature space tremendously and result in very high computational complexity.\n",
    "\n",
    "Top 4 models based on performance: Gradient Boosting, Adaboost, random Forest and Logistic regression. \n",
    "Gradient Boosting Classifier  high training time, but still managable, low prediction time, and superior performance.  \n",
    "Adaboost Classifier , has  lower training time, slightly bigger prediction time and superior performance. Moreover, Adaboost has less parameters than Gradient Boosting Classifier and is much easier to tune.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Parameter Tuning for AdaBosst Classifier\n",
    "<a id=\"tuneA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost classifier doesn't have much parameters to tune, however, it is critical to find proper balance between n-estimators and learning rate. We will try to increase n-estimators and decrease learning rate slowly until we observe no further improvement in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.metrics import make_scorer \n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(random_state = random_state)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "parameters = {'n_estimators': range(20,1021,100)}\n",
    "\n",
    "#Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions_val = best_clf.predict(X_val)\n",
    "best_predictions_train = best_clf.predict(X_train)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\n",
    "print(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\n",
    "print(\"Optimal parameters:\", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning rate = 1 (default parmeter for Adaboost) optimal n_estimators is 1020- that is on the edge of search parameters. I will increase n_estimators further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(random_state = random_state)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "parameters = { 'n_estimators': range(1000,1501,100)}\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions_val = best_clf.predict(X_val)\n",
    "best_predictions_train = best_clf.predict(X_train)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\n",
    "print(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\n",
    "print(\"Optimal parameters:\", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://)For learning rate = 1 (default parmeter for Adaboost) optimal n_estimators is 1200. In order to make model better I will try to further decrease learning rate by half and increase n_estimates a liitle bit more than twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(random_state = random_state)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "parameters = { 'n_estimators': range(2000,3001,200),\n",
    "              'learning_rate': [0.5]}\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions_val = best_clf.predict(X_val)\n",
    "best_predictions_train = best_clf.predict(X_train)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\n",
    "print(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\n",
    "print(\"Optimal parameters:\", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance did not improve by decreasing learning rate and increasing n_estimators. I will use learning_rate of 1 and n_estimators of 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=4000,random_state = random_state, learning_rate = 0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "best_predictions_val_ab = clf.predict(X_val)\n",
    "best_predictions_train_ab = clf.predict(X_train)\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val_ab)))\n",
    "print(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train_ab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model with optimal parameters found during gridsearch:\n",
    "clf_AB = AdaBoostClassifier(n_estimators=1200,random_state = random_state)\n",
    "clf_AB.fit(X_train, y_train)\n",
    "# predict outcome using predict_probe instead of predict function:\n",
    "probs_train_ab = clf_AB.predict_proba(X_train)[:, 1]\n",
    "probs_val_ab = clf_AB.predict_proba(X_val)[:, 1]\n",
    "print(\"score train: {}\".format(roc_auc_score(y_train, probs_train_ab)))\n",
    "print(\"score validation: {}\".format(roc_auc_score(y_val, probs_val_ab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Parameter Tuning for Gradient Boosting Classifier\n",
    "<a id=\"tuneB\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Tuning Strategy:\n",
    "\n",
    "Gradient Boosting Classifier model is prone to overfit and needs smart parameter tuning. As the model has a high training computational complexity I will perforsm parameters tuning in steps, to make it faster: \n",
    "\n",
    "1. I will freeze learning_rate and all tree parameters of the model, and search for the optimal n_estimates for this learning rate\n",
    "2. Experiment with tree parameters to get optimal setting (max_depth, max_samples_leaf, min_samples_split, min_weight_fraction_leaf, max_leaf_nodes, max_features)\n",
    "3. Search for optimal 'subsample' \n",
    "4. Lower the learning rate and increase the estimators proportionally to get more robust models \n",
    "\n",
    "\n",
    "References: \n",
    "[Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1. Search optimal n_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "clf = GradientBoostingClassifier(random_state = random_state)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "parameters = {  'n_estimators': range(20,101,20),\n",
    "                'learning_rate':[0.2],\n",
    "                'min_samples_split': [500],\n",
    "                'min_samples_leaf' : [50],\n",
    "                'max_depth' : [8],\n",
    "                'subsample' : [0.8]}\n",
    "\n",
    "#Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,verbose=1, n_jobs =-1)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions_val = best_clf.predict(X_val)\n",
    "best_predictions_train = best_clf.predict(X_train)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\n",
    "print(\"Optimal parameters:\", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal n_estimators = 80 for learning_rate = 0.2. We will freezee this values for now and proceed to tree parameters grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2. Search for optimal Tree parameters\n",
    "Now lets move onto tuning the tree parameters. I plan to do this in following stages:\n",
    "\n",
    "Tune max_depth and num_samples_split\n",
    "Tune min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "clf = GradientBoostingClassifier(random_state = random_state)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed\n",
    "parameters = {'max_depth':range(2,12,2), \n",
    "              'min_samples_split':range(100,601,100),\n",
    "              'n_estimators': [80],\n",
    "              'learning_rate':[0.2],                \n",
    "              'min_samples_leaf' : [50],\n",
    "              'subsample' : [0.8]\n",
    "              }\n",
    "\n",
    "#Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions_val = best_clf.predict(X_val)\n",
    "best_predictions_train = best_clf.predict(X_train)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\n",
    "print(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\n",
    "print(\"Optimal parameters:\", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal max_depth = 6 and  min_samples_split =200 . We will freezee this values and proceed to min_samples_leaf grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "clf = GradientBoostingClassifier(random_state = random_state)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "parameters = {'min_samples_leaf':range(10,71,10),\n",
    "              'max_depth': [6], \n",
    "              'min_samples_split': [200],\n",
    "              'n_estimators': [80],\n",
    "              'learning_rate':[0.2],\n",
    "              'subsample' : [0.8]}\n",
    "\n",
    "#Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer, verbose=1, n_jobs =-1)\n",
    "\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions_val = best_clf.predict(X_val)\n",
    "best_predictions_train = best_clf.predict(X_train)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\n",
    "print(\"Optimal parameters:\", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal min_samples_leaf = 50. We will freezee this value and proceed to subsamples grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3. Search for optimal subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "clf = GradientBoostingClassifier(random_state = random_state)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "parameters = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],\n",
    "              'min_samples_leaf': [50],\n",
    "              'max_depth': [6], \n",
    "              'min_samples_split': [200],\n",
    "              'n_estimators': [80],\n",
    "              'learning_rate':[0.2]}\n",
    "\n",
    "#Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer, verbose=1, n_jobs =-1)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions_val = best_clf.predict(X_val)\n",
    "best_predictions_train = best_clf.predict(X_train)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\n",
    "print(\"Optimal parameters:\", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal subsample = 0.8. We will freezee this value and proceed to finding optimal learning_rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4.Find optimal learning_rate\n",
    "Now lets reduce learning rate to half of the original value, i.e. 0.1. To match with learning rate n_estomators need to be increased approximatelly twice, i.e. values around 160 and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "clf = GradientBoostingClassifier(random_state = random_state)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "parameters = {'subsample':[0.8],\n",
    "              'min_samples_leaf': [50],\n",
    "              'max_depth': [6], \n",
    "              'min_samples_split': [200],\n",
    "              'n_estimators': range(140, 241, 20),\n",
    "              'learning_rate':[0.1]}\n",
    "\n",
    "#Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer, verbose=1, n_jobs =-1)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions_val = best_clf.predict(X_val)\n",
    "best_predictions_train = best_clf.predict(X_train)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\n",
    "print(\"Optimal parameters:\", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50% decrease of learning rate improved model performance of validation set. We will try to decrease learning rate more, i.e to 0.05 and increase n_estimators slightly more tham twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "clf = GradientBoostingClassifier(random_state = random_state)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "parameters = {'subsample':[0.8],\n",
    "              'min_samples_leaf': [50],\n",
    "              'max_depth': [6], \n",
    "              'min_samples_split': [200],\n",
    "              'n_estimators': range(360, 401, 20) ,\n",
    "              'learning_rate':[0.05]}\n",
    "\n",
    "#Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer, verbose=1, n_jobs =-1)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions_val = best_clf.predict(X_val)\n",
    "best_predictions_train = best_clf.predict(X_train)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\n",
    "print(\"Optimal parameters:\", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](http://)With lower learning rate performance is worse, as the model is overfitting to the training set. Therefore we will leave learning rate of 0.1 and n_estimators = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting GradientBoostingClassifier with optimal parameters:\n",
    "clf_GB = GradientBoostingClassifier(random_state = random_state, subsample = 0.8, min_samples_leaf = 50,\n",
    "              max_depth = 6, min_samples_split = 200, n_estimators = 180, learning_rate = 0.1 )\n",
    "clf_GB.fit(X_train, y_train)\n",
    "\n",
    "best_predictions_val_gb = clf_GB.predict(X_val)\n",
    "best_predictions_train_gb = clf_GB.predict(X_train)\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val_gb)))\n",
    "print(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train_gb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict outcome using predict_probe instead of predict function:\n",
    "probs_train_gb = clf_GB.predict_proba(X_train)[:, 1]\n",
    "probs_val_gb = clf_GB.predict_proba(X_val)[:, 1]\n",
    "print(\"score train: {}\".format(roc_auc_score(y_train, probs_train_gb)))\n",
    "print(\"score test: {}\".format(roc_auc_score(y_val, probs_val_gb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Logistic Regression\n",
    "<a id=\"logistic\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state= random_state)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "parameters = { 'C': [0.001, 0.01, 0.05, 0.1, 0.5, 0.7, 0.8, 0.9, 1, 5, 10, 20, 50]}\n",
    "\n",
    "#Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf_LR = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions_val_lr = clf_LR.predict(X_val)\n",
    "best_predictions_train_lr = clf_LR.predict(X_train)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val_lr)))\n",
    "print(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train_lr)))\n",
    "print(\"Optimal parameters:\", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict outcome using predict_probe instead of predict function:\n",
    "probs_train_lr = clf_LR.predict_proba(X_train)[:, 1]\n",
    "probs_val_lr = clf_LR.predict_proba(X_val)[:, 1]\n",
    "print(\"score train: {}\".format(roc_auc_score(y_train, probs_train_lr)))\n",
    "print(\"score test: {}\".format(roc_auc_score(y_val, probs_val_lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Random Forest\n",
    "<a id=\"forest\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state= random_state)\n",
    "\n",
    "# Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "parameters = { 'n_estimators': range(20,1020,100),\n",
    "                'max_depth': range(2, 10, 1)}\n",
    "\n",
    "#Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf_RF = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions_val_rf = clf_RF.predict(X_val)\n",
    "best_predictions_train_rf = clf_RF.predict(X_train)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val_rf)))\n",
    "print(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train_rf)))\n",
    "print(\"Optimal parameters:\", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict outcome using predict_probe instead of predict function:\n",
    "probs_train_rf = clf_RF.predict_proba(X_train)[:, 1]\n",
    "probs_val_rf = clf_RF.predict_proba(X_val)[:, 1]\n",
    "print(\"score train: {}\".format(roc_auc_score(y_train, probs_train_rf)))\n",
    "print(\"score test: {}\".format(roc_auc_score(y_val, probs_val_rf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. Comparison of Classifiers\n",
    "<a id=\"compare\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AdaBoost classifier only has couple of parameters to tune, therefore, it was much faster to find optimal parameters than for Gradiend Boosting.  \n",
    "2. In term of performance, Gradient Boosting achieved better performance.\n",
    "3. Logistic regression is performing worse than tree based ensemble models, however, it can be useful to combine it with tree based models as it has very different approach to prediction and can potentially catch dependencies that three based models can't\n",
    "\n",
    "I will try three subbmissions:\n",
    "    - top model: GB\n",
    "    - average between top 2 models: GB and AB\n",
    "    - average between top 3 models: GB, AB and RF\n",
    "    - average between all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"score train for Adaboost: {}\".format(roc_auc_score(y_train, probs_train_ab)))\n",
    "print(\"score test for Adaboost: {}\".format(roc_auc_score(y_val, probs_val_ab)))\n",
    "print(\"score train for Gradient Boosting: {}\".format(roc_auc_score(y_train, probs_train_gb)))\n",
    "print(\"score test for Gradient Boosting: {}\".format(roc_auc_score(y_val, probs_val_gb)))\n",
    "print(\"score train for Logistic Regression: {}\".format(roc_auc_score(y_train, probs_train_lr)))\n",
    "print(\"score test for Logistic Regression: {}\".format(roc_auc_score(y_val, probs_val_lr)))\n",
    "print(\"score train for Random Forest: {}\".format(roc_auc_score(y_train, probs_train_rf)))\n",
    "print(\"score test for Random Forest: {}\".format(roc_auc_score(y_val, probs_val_rf)))\n",
    "print(\"score train for top2 models: {}\".format(roc_auc_score(y_train, (probs_train_gb+probs_train_ab)/2)))\n",
    "print(\"score test for top2 models: {}\".format(roc_auc_score(y_val, (probs_val_gb+probs_val_ab)/2)))\n",
    "print(\"score train for top3 models: {}\".format(roc_auc_score(y_train, (probs_train_gb+probs_train_ab+probs_train_rf)/3)))\n",
    "print(\"score test for top3 models: {}\".format(roc_auc_score(y_val, (probs_val_gb+probs_val_ab+probs_val_rf)/3)))\n",
    "print(\"score train for all models: {}\".format(roc_auc_score(y_train, (probs_train_gb+probs_train_ab+\n",
    "                                                                      probs_train_rf+probs_train_lr)/4)))\n",
    "print(\"score test for all models: {}\".format(roc_auc_score(y_val, (probs_val_gb+probs_val_ab+\n",
    "                                                                      probs_val_rf+ probs_val_lr)/4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "\n",
    "The best performance can be achieved by blend of best two models models: Gradient Boosting and Adaboost.   \n",
    "I will try to submit all four combinations though, to prove the concept and see how consistent is public leaderbord results with cross validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Preprocessing of Test Data\n",
    "<a id=\"test\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of data_test to overwrite duting feature engineering:\n",
    "X_test = data_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data has multiple missing values that needs to be filled. \n",
    "For different variable types we will use different missing value strategy:\n",
    "1. Numeric variables with appr gaussian disctribution: fill missing values with mean value from train set\n",
    "2. Numeric variables with skewed dictribution: fill missing values with median values from train set\n",
    "3. Catogorical data: fill values with most frequent cateory in train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values for numeric variables with approximatelly gaussian dictribution:\n",
    "for col in ['age', 'education-num', 'hours-per-week']:\n",
    "    X_test[col]= X_test[col].fillna(data_train[col].mean())\n",
    "\n",
    "# fill missing values for numeric variables with skewed dictribution:\n",
    "for col in ['capital-gain', 'capital-loss']:\n",
    "    X_test[col]= X_test[col].fillna(data_train[col].median())\n",
    "\n",
    "#fill missing categorical values with most freaquent category:\n",
    "for col in categorical:\n",
    "    X_test[col]= X_test[col].fillna(data_train.groupby([col])[col].count().sort_values(ascending=False).index[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values in X_test after filling them in:\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values in X_test -->success\n",
    "\n",
    "Now we need to log transform, scale and convert X_test features to dummies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transform skewed data\n",
    "X_test[skewed] = log_transform(X_test)\n",
    "\n",
    "#scale continues variables:\n",
    "X_test[continues]= normalize(X_test)\n",
    "\n",
    "# One-hot encode thedata using pandas.get_dummies()\n",
    "X_test_final = pd.get_dummies(X_test)\n",
    "\n",
    "# Print the number of features after one-hot encoding\n",
    "encoded = list(X_test_final.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Prediction and Submission\n",
    "<a id=\"pred\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf_GB\n",
    "test = pd.read_csv(\"../input/test_census.csv\")\n",
    "\n",
    "test['id'] = test.iloc[:,0] \n",
    "test['income'] = best_model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "test[['id', 'income']].to_csv(\"submissionGB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf_AB\n",
    "test = pd.read_csv(\"../input/test_census.csv\")\n",
    "\n",
    "test['id'] = test.iloc[:,0] \n",
    "test['income'] = best_model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "test[['id', 'income']].to_csv(\"submissionAB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf_LR\n",
    "test = pd.read_csv(\"../input/test_census.csv\")\n",
    "\n",
    "test['id'] = test.iloc[:,0] \n",
    "test['income'] = best_model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "test[['id', 'income']].to_csv(\"submissionLR.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf_RF\n",
    "test = pd.read_csv(\"../input/test_census.csv\")\n",
    "\n",
    "test['id'] = test.iloc[:,0] \n",
    "test['income'] = best_model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "test[['id', 'income']].to_csv(\"submissionRF.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/test_census.csv\")\n",
    "\n",
    "test['id'] = test.iloc[:,0] \n",
    "test['income'] = (clf_GB.predict_proba(X_test_final)[:, 1] + clf_AB.predict_proba(X_test_final)[:, 1])/2\n",
    "\n",
    "test[['id', 'income']].to_csv(\"submission_top2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/test_census.csv\")\n",
    "\n",
    "test['id'] = test.iloc[:,0] \n",
    "test['income'] = (clf_GB.predict_proba(X_test_final)[:, 1] + clf_AB.predict_proba(X_test_final)[:, 1]+\n",
    "                  clf_RF.predict_proba(X_test_final)[:, 1])/3\n",
    "\n",
    "test[['id', 'income']].to_csv(\"submission_top3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/test_census.csv\")\n",
    "\n",
    "test['id'] = test.iloc[:,0] \n",
    "test['income'] = (clf_GB.predict_proba(X_test_final)[:, 1] + clf_AB.predict_proba(X_test_final)[:, 1]+\n",
    "                clf_RF.predict_proba(X_test_final)[:, 1] + clf_LR.predict_proba(X_test_final)[:, 1])/4\n",
    "\n",
    "test[['id', 'income']].to_csv(\"submission_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: Results and Conclusion\n",
    "<a id=\"results\"></a>\n",
    "\n",
    "### Score on the Public Leaderboard\n",
    "\n",
    "1. Best model: 0.92542\n",
    "\n",
    "2. average between top 2 models: 0.93687 \n",
    "\n",
    "3. average between top 3 models: 0.93160\n",
    "\n",
    "4. average between all models: 0.93684\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The best performance on Public Leaderbord is achieved by combination of best two models models: Gradien boost and Ada Boost. This result is consistent with result achived during crossvalidation. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
